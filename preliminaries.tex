\section{Preliminaries}

\paragraph{Setting and notation} Consider a $K$-armed bandits $(F_1,\dots, F_K) \in \cF^K$, where $\cF$ is a family of distributions. At each time step $t$, an arm $A_t$ is pulled and a reward $r_t\in\R$ is observed. Before selecting the next arm, the learner updates her policy considering the past observations. The policy thus depend on $\cH_{t}=(A_1,r_1,\dots, A_t, r_t)$.

Consider a randomized policy that samples an arm at time $t$ with probability $p_t \in \Delta_K$, that is for all $k \in [K]$, $\bP(A_{t+1}=k | \cH_{t-1})=p_{t, k}$. We want the policy to satisfy the fairness constraint

\[\forall k \in [K]\;, \; \liminf \frac{1}{T} \bE\left[\sum_{t=1}^T p_{t,k} \mu_k \right] \geq \lambda_k \;,\]

i.e. if $\lambda_k>0$ we want to guarantee a minimum expected reward to arm $k$ on the long run. If an algorithm satisfies this constraint, then we want to minimize the regret, that is defined by in the class of fair algorithms:

\[ \cR_T = \sum_{t=1}^T \bE\left[(p^\star - p_t)^\top\mu \right]\;, \]

where $p^\star$ is one of the optimal allocation among those that satisfy the constraint (not unique if several arms are optimal). It is more illustrative to write the regret only in terms of the sub-optimal arm. Indeed, for any $i\in [K]$ it holds that $p_i = 1-\sum_{j \neq i } p_j$. Assuming w.l.o.g. that arm $1$ is among the optimal arms, we obtain that 

\begin{equation}\label{eq::regret}\cR_T =  \sum_{k=2}^K \bE\left[\sum_{t=1}^T (p_{k,t} - p^\star_k)\right] \Delta_k \;, \end{equation}

where $\Delta_k = \mu_k - \mu_1$. This expression is closer to usual formulations of the regret. In fact, if $p_k^\star=0$ (no requirement for arm $k$) then we obtain that the contribution of arm $k$ to the regret is $\bE[N_k(t)] \Delta_k$, which is the same term as for standard MAB. In the following we call $\R_T$ the \emph{bandit regret} (the bandit wants to maximize profit). 

For simplicity, we also define a \emph{fair regret} as follows, 

\[ \cV_T = \max_{k \in [K]} \bE\left[\sum_{t=1}^T \left(\lambda_k-p_{k,t} \mu_k \right)\right] \;. \]

There is a natural trade-off between $\cR_T$ and $\cV_t$. For instance, an unfair bandit policy may achieve negative $\cR_T$ in our setting (with $\lim p_{t,k}=0$ for sub-optimal arm), at the cost of some linear fair regrets. In this paper, one of our objectives is to identify the best achievable trade-offs. As a comparison, a recent paper from XXX obtain $\cO(T^{3/4})$ for both regrets.


\paragraph{Feasibility} It is clear that for any optimal allocation $p^\star$ and arm $k$ it holds that $p_k^\star\geq q_k^\star \coloneqq \frac{\lambda_k}{\mu_k}$, with equality if $k$ is sub-optimal. If this allocation is possible (i.e $\sum_k q_k^\star\leq 1$) we say that the problem is \emph{feasible}, otherwise we call it \emph{in-feasible}. We verify easily that 
\[\text{Fair MAB is feasible } \Longleftrightarrow \sum_{k=1}^K \frac{\lambda_k}{\mu_k}\leq 1. \]
In this paper, we will generally assume that the learner is guaranteed that the problem is feasible, which gives some information on the arms: if the arms satisfy $\max \mu_k \leq 1$, we have for instance that for any arm $k$ satisfying $\lambda_k>0$ it both holds that $p_k^\star>\lambda_k$ and $\mu_k \geq \frac{\lambda_k}{1-\sum_{i\neq k}^K \lambda_i}$. 

However, we can also imagine a scenario where the feasibility cannot be guaranteed beforehand. In that case, we assume that the learner will try to adjust her policy in order to target the ``closest'' feasible instance to the initial objective, that we denote by $p_k^{\star, \text{UF}}$ and define by  
\[p_k^{\star, \text{UF}}= \frac{\frac{\lambda_k}{\mu_k}}{\sum_{j=1}^K\frac{\lambda_j}{\mu_j}}.\]
We will propose an algorithm that can satisfy this objective in Section XX.

In the rest of the paper, we call $\rho_\lambda = 1-\sum_{j=1}^K\frac{\lambda_j}{\mu_j}$ the \emph{feasibility} gap of the fair MAB problem. 