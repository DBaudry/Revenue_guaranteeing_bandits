\section{Lower bounds}\label{sec::lower_bounds}

Trivial remark: since the standard MAB is a subcase all the usual lower bounds still hold. In the feasible case $\rho_\lambda>1$ one cannot hope to do better than the $\sqrt{(K-1)T}$, because the best arm has to be discovered in order to allocate the remaining mass appropriately.

\paragraph{Intuitions} Assume $\forall j, \lambda_j >0$. For some $k$, we may try find lower bound on $|\sum_{t=1}^T (x_{k, t}-x_k^\star)|$, since we saw that it is possible to make one of the two regrets negative but in all of our results we get $\cO(\sqrt{T\log(T)})$ on this quantity. 

We can design hard problems, e.g. with $2$ arms, $(\lambda_1, \mu_1)=0, 1$, while  $(\lambda_2, \mu_2)=(\epsilon, 2\epsilon)$. A problem with $(\lambda_2, \mu_2)=(\epsilon, 3\epsilon)$ or $(\lambda_2, \mu_2)=(\epsilon, \epsilon)$ leads to a very different optimal allocation ($x_2^\star=1/2$, $1/3$ and $1$ respectively). $\epsilon=1/T$ is too small to distinguish in which setting we are, we have linear regret. This hints that for non vacuous bounds we need to consider $\lambda$ as fixed and independent of $T$.

In that case, we can obtain with standard tools a lower bound of order $\left(\frac{\lambda_k}{\mu_k^2}\sqrt{T} \right)$, which is what we expect since it is the scaling of $\left|\frac{\lambda_k}{\mu_k}-\frac{\lambda_k}{\mu_k+\cO(T^{-1/2})}\right|$. \textcolor{blue}{Write this formally}. If we consider this result, the optimality ratio of our bounds is $\sqrt{\frac{\log(T)}{\alpha_k}}=\sqrt{\frac{\log(T)\mu^+}{\lambda_k}}$.

Note that this bound is a minimax bound if we consider $(\lambda_k)_{k \in [K]}$ as constants. Indeed, we proved that in the feasible case it holds that $\mu_k \geq \frac{\lambda_k}{1- \sum_{i\neq k} \frac{\lambda_i}{\mu^+}}$, so we can also write the lower bound as $\Omega\left(\frac{(1- \sum_{i\neq k} \frac{\lambda_i}{\mu^+})^2}{\lambda_k} \sqrt{T}\right)$. Or to be less precise we can just say that $\mu_k \geq \lambda_k$ (without any assumption) and directly get $\Omega\left(\frac{\sqrt{T}}{\lambda_k}\right)$.

\paragraph{Formal result} We prove the following lower bound. 

\begin{theorem}
	There exists a two-armed bandit problem for which
	\[ \max\left\{\frac{\cV_T}{\mu}, \frac{\cR_T}{\Delta}  \right\} \geq \frac{e^{-\frac{1}{2}}}{12}\times \frac{\lambda}{\mu^2} \sqrt{\frac{T}{\lambda/\mu}}\;. \]
\end{theorem}

\begin{proof}
	Let us consider a simple setting with 2 Bernoulli arms. Assume that arm $2$ has mean $\mu \in (0,1)$ and arm $1$ has mean $\mu+\Delta$ for some $\Delta \in (0, 1-\mu)$. Further assume that $\lambda_1 = 0$ and $\lambda_2 \coloneqq \lambda >0$. Let us also denote by $p_t$ and $p^\star$ respectively the sampling probability of arm $2$ at times $t$ and $p^\star=\frac{\lambda}{\mu}$, assumed to be feasible. First, we remark that $\cR_T=\Delta \bE\left[\sum_{t=1}^T (p_t-p^\star) \right]$ and $\cV_T = \mu\bE\left[\sum_{t=1}^T (p^\star-p_t) \right]$. We thus obtain that
	\[ \max\left\{\frac{\cV_T}{\mu}, \frac{\cR_T}{\Delta}  \right\} \geq \bE[\cS_t] \coloneqq  \left|\bE\left[\sum_{t=1}^T (p^\star-p_t) \right]\right| \;, \]
	which becomes the quantity of interest in the following.
	
	We use the Bretagnolle-Huber inequality (see e.g. Theorem 14.2 of \cite{BanditBook}). Denoting the initial model by $\nu$, we define an alternative model $\nu'$ for which arm $1$ is identical as in $\nu$, but arm $2$ has a mean $\mu+\epsilon$, for $\epsilon\leq \Delta$ (we could also consider $\mu-\epsilon$ for $\epsilon \leq\mu-\lambda$). We then define the event \[A = \left\{ \sum_{t=1}^T p_t \leq \frac{\lambda}{\mu+\frac{\epsilon}{2}} \right\} \;. \]
If event $A$ holds, $\cS_t$ is linear under model $\nu$. More precisely,
\[ A \text{ holds under } \nu \Rightarrow \cS_t \geq T\lambda \frac{\epsilon}{2\mu\left(\mu+\frac{\epsilon}{2}\right)} \;. \]
Similarly, $\cS_t$ is linear under model $\nu'$ when $A^c$ holds, because
\[ A^c \text{ holds under } \nu' \Rightarrow \cS_t \geq T\lambda \frac{\epsilon}{2(\mu+\epsilon)\left(\mu+\frac{\epsilon}{2}\right)} \;. \]
Combining these two results and using Bretagnolle-Hubert inequality, we obtain that 
\begin{align*}
\bE_\nu[\cS_t] + \bE_{\nu'}[\cS_t] & \geq T\lambda \frac{\epsilon}{2(\mu+\epsilon)\left(\mu+\frac{\epsilon}{2}\right)}\left(\bP_\nu(A) + \bP_{\nu'}(A^c)\right) \\
& \geq T\lambda \frac{\epsilon}{2(\mu+\epsilon)\left(\mu+\frac{\epsilon}{2}\right)} \exp\left(- \bE_{\nu}[N_2(T)] \kl(\mu, \mu+\epsilon)\right) \\
& \geq T\lambda \frac{\epsilon}{2(\mu+\epsilon)\left(\mu+\frac{\epsilon}{2}\right)} \exp\left(-2\epsilon^2 \times T p^\star \right) \;,
\end{align*}
since we consider this lower bound on the class of fair algorithm (we could have used $\bE_\nu[N_2(T)]\leq T$ otherwise). Then, let us consider $\epsilon = \frac{1}{2\sqrt{Tp^\star}}\vee \mu$. In that case, we obtain 
\[\bE_\nu[\cS_t] + \bE_{\nu'}[\cS_t] \geq \frac{e^{-\frac{1}{2}} }{12} \times \frac{\lambda}{\mu^2} \sqrt{\frac{T}{p^\star}}\;. \]

\end{proof}

We can make several remarks. First, this bound may not look like a problem-independent bound at first sight (due to the scaling in $\mu$) but under the assumption that the problem is feasible and that $\lambda$ is fixed we know that $\mu\geq \lambda$ and so we can obtain that 
\[\frac{\lambda}{\mu^2 \sqrt{p^\star}} \in \left[\sqrt{\lambda}, \frac{1}{\lambda}\right] \;, \] 
which is independent of the means, and provide a lower bound of order $\Omega(\sqrt{\lambda T})$. As explained in our example above it is impossible to remove a dependency in either $\lambda$ or $\mu$. The lower bound formalizes this: we see that if for instance $\mu=\frac{1}{2} \lambda$ and $\lambda= \cO(T^{-1/2})$ the constant is of order $\lambda^{-1}$ and the lower bound becomes $\Omega(T)$.

\db{To Hugo and Mathieu: please check those results. They look coherent to me, and provide a nice story on the regrets trade-off.}